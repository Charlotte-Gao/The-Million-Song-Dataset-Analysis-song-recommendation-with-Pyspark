#Data processing#q1.(a) Give an overview of the structure of the datasets, including file formats, data types, and the expected level of parallelism that# you can expect to achieve from HDFS(only read a subset of lager datasets)hdfs dfs -ls /data/msd/ #all the data hadoop fs -du -s -h /data/msd/ #size of all the data (human readable version)hadoop fs -du -s -h /data/msd/audio  #size of audiohadoop fs -du -s -h /data/msd/genre  #size of genrehadoop fs -du -s -h /data/msd/main  #size of mainhadoop fs -du -s -h /data/msd/tasteprofile  #size of tasteprofile#tree-directory hadoop fs -lsr /data/msd | awk '{print $8}' | \sed -e 's/[^-][^\/]*\//--/g' -e 's/^/ /' -e 's/-/|/'#count of rows hdfs dfs -cat /data/msd/main/summary/metadata.csv.gz | gunzip | wc -l #metadatahdfs dfs -cat /data/msd/audio/features/msd-ssd-v1.0.csv/* | gunzip | wc -l#audio featureshdfs dfs -cat /data/msd/audio/statistics/sample_properties.csv.gz | gunzip | wc -l #audio additional track statisticshdfs dfs -cat /data/msd/genre/msd-topMAGD-genreAssignment.tsv | wc -l #genre counthdfs dfs -cat /data/msd/tasteprofile/triplets.tsv/* | gunzip | wc -l#q2.(a) Filter the Taste Profile dataset to remove the songs which were mismatched.# Python and pyspark modules requiredimport sysfrom pyspark import SparkContextfrom pyspark.sql import SparkSessionfrom pyspark.sql.types import *from pyspark.sql.functions import *# Required to allow the file to be submitted and run using spark-submit instead# of using pyspark interactivelyspark = SparkSession.builder.getOrCreate()sc = SparkContext.getOrCreate()mismatches_schema = StructType([    StructField("song_id", StringType(), True),    StructField("song_artist", StringType(), True),    StructField("song_title", StringType(), True),    StructField("track_id", StringType(), True),    StructField("track_artist", StringType(), True),    StructField("track_title", StringType(), True)])with open("/scratch-network/courses/2020/DATA420-20S1/data/msd/tasteprofile/mismatches/sid_matches_manually_accepted.txt", "r") as f:    lines = f.readlines()    sid_matches_manually_accepted = []    for line in lines:        if line.startswith("< ERROR: "):            a = line[10:28]            b = line[29:47]            c, d = line[49:-1].split("  !=  ")            e, f = c.split("  -  ")            g, h = d.split("  -  ")            sid_matches_manually_accepted.append((a, e, f, b, g, h))matches_manually_accepted = spark.createDataFrame(sc.parallelize(sid_matches_manually_accepted, 8), schema=mismatches_schema)matches_manually_accepted.cache()matches_manually_accepted.show(10, 40)print(matches_manually_accepted.count())  # 488with open("/scratch-network/courses/2020/DATA420-20S1/data/msd/tasteprofile/mismatches/sid_mismatches.txt", "r") as f:    lines = f.readlines()    sid_mismatches = []    for line in lines:        if line.startswith("ERROR: "):            a = line[8:26]            b = line[27:45]            c, d = line[47:-1].split("  !=  ")            e, f = c.split("  -  ")            g, h = d.split("  -  ")            sid_mismatches.append((a, e, f, b, g, h))mismatches = spark.createDataFrame(sc.parallelize(sid_mismatches, 64), schema=mismatches_schema)mismatches.cache()mismatches.show(10, 40)mismatches.count()  # 19094triplets_schema = StructType([    StructField("user_id", StringType(), True),    StructField("song_id", StringType(), True),    StructField("plays", IntegerType(), True)])triplets = (    spark.read.format("csv")    .option("header", "false")    .option("delimiter", "\t")    .option("codec", "gzip")    .schema(triplets_schema)    .load("hdfs:///data/msd/tasteprofile/triplets.tsv/")    .cache())triplets.cache()triplets.show(10, 50)mismatches_not_accepted = mismatches.join(matches_manually_accepted, on="song_id", how="left_anti")triplets_not_mismatched = triplets.join(mismatches_not_accepted, on="song_id", how="left_anti")print(triplets.count())                 # 48373586print(triplets_not_mismatched.count())  # 45795111#(b) Load the audio feature attribute names and types from the audio/attributes directory.hdfs dfs -cat "/data/msd/audio/attributes/*" | awk -F',' '{print $2}' | sort | uniqaudio_attribute_type_mapping = {  "NUMERIC": DoubleType(),  "real": DoubleType(),  "string": StringType(),  "STRING": StringType()}audio_dataset_names = [  "msd-jmir-area-of-moments-all-v1.0",  "msd-jmir-lpc-all-v1.0",  "msd-jmir-methods-of-moments-all-v1.0",  "msd-jmir-mfcc-all-v1.0",  "msd-jmir-spectral-all-all-v1.0",  "msd-jmir-spectral-derivatives-all-all-v1.0",  "msd-marsyas-timbral-v1.0",  "msd-mvd-v1.0",  "msd-rh-v1.0",  "msd-rp-v1.0",  "msd-ssd-v1.0",  "msd-trh-v1.0",  "msd-tssd-v1.0"]audio_dataset_schemas = {}for audio_dataset_name in audio_dataset_names:  print(audio_dataset_name)  audio_dataset_path = f"/scratch-network/courses/2020/DATA420-20S1/data/msd/audio/attributes/{audio_dataset_name}.attributes.csv"  with open(audio_dataset_path, "r") as f:    rows = [line.strip().split(",") for line in f.readlines()]  # rows[-1][0] = "track_id"  # for i, row in enumerate(rows[0:-1]):  #   row[0] = f"feature_{i:04d}"  audio_dataset_schemas[audio_dataset_name] = StructType([    StructField(row[0], audio_attribute_type_mapping[row[1]], True) for row in rows  ])  #----------------------------------------------------------------------------audio similiarity# Pick one of the small datasets to get started.# "msd-jmir-method-of-moments-all-v1.0"audio_dataset_name = "msd-jmir-methods-of-moments-all-v1.0"#load dataMethod_of_Moments = (    spark.read.format("csv")    .option("header", "false")    .option("delimiter", ",")    .option("codec", "gzip")    .schema(audio_dataset_schemas[audio_dataset_name])    .load(f"hdfs:///data/msd/audio/features/{audio_dataset_name}.csv/"))Method_of_Moments.show()Method_of_Moments .cache()Method_of_Moments .take(1)#rename track columnMethod_of_Moments  = Method_of_Moments .withColumnRenamed('MSD_TRACKID', 'track_id')#remove the single quotesimport pyspark.sql.functions as FMethod_of_Moments  = Method_of_Moments .withColumn('track_id', F.regexp_replace('track_id', "\'", ''))from pyspark.ml.stat import Correlationfrom pyspark.ml.feature import VectorAssembler# the descriptive statistics for each feature columnstatistics = (    Method_of_Moments     .select([col for col in Method_of_Moments .columns if col.startswith("M")])    .describe()    .toPandas()    .set_index("summary")    .rename_axis(None)    .T)print(statistics)#numeric feature distribution Produce descriptive statistics for each feature column in the dataset you picked. Are any features strongly correlated?#correlation checkassembler = VectorAssembler(    inputCols=[col for col in Method_of_Moments .columns if col.startswith("M")],    outputCol="Features")features = assembler.transform(Method_of_Moments ).select(["track_id", "Features"])features.show()features.cache()features.count()features.show(10, 100)correlations = Correlation.corr(features, 'Features', 'pearson').collect()[0][0].toArray()for i in range(0, correlations.shape[0]):    for j in range(i + 1, correlations.shape[1]):        if correlations[i, j] > 0.8:            print((i, j))          #(b) Load the MSD All Music Genre Dataset (MAGD).# load genre dataschema_genre = StructType([    StructField('track_id', StringType()),    StructField('GENRE', StringType()),    ])genre = (    spark.read.format("com.databricks.spark.csv")    .option("header", "false")    .option("inferSchema", "false")    .option("delimiter", "\t")    .schema(schema_genre)    .load("/data/msd/genre/msd-MAGD-genreAssignment.tsv")    )#remove the mismatchesgenre = genre.join(mismatches_not_accepted, on="track_id", how="left_anti")genre.groupBy('GENRE').count().orderBy('count', ascending=False).show()#Visualize the distribution of genres for the songs that were matched.import matplotlib.pyplot as pltgenre_distribution = genre \    .groupBy('GENRE') \    .count() \    .orderBy('count', ascending=False) \    .toPandas()genre_distribution.index = genre_distribution.GENREplt.clf()plt.figure(figsize=(15, 15))genre_distribution['count'].plot.pie()plt.legend()plt.title('Distribution of Genres')plt.xlabel('')plt.ylabel('')plt.savefig('genre_distribution.jpg')#(c) Merge the genres dataset and the audio features dataset so that every song has a label.Method_of_Moments=Method_of_Moments.join(mismatches_not_accepted, on="track_id", how="left_anti")features_with_labels = genre.join(Method_of_Moments, on='track_id',how="inner")features_with_labels.show()#Q2. develop a binary claffication model#drop track_id and choose "rap'dataset= features_with_labels.withColumn('label', F.when(F.col('GENRE') == 'Rap', 1).otherwise(0))dataset.groupBy('label').count().show()dataset.show()dataset=dataset.drop("track_id")#import functions neededfrom pyspark.ml.feature import VectorAssembler, PCA, StringIndexer, StandardScalerfrom pyspark.ml import Pipelinefrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes, MultilayerPerceptronClassifier, GBTClassifierfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluatorfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidatordef print_class_balance(data, name):    N = data.count()    counts = data.groupBy("label").count().toPandas()    counts["ratio"] = counts["count"] / N    print(name)    print(N)    print(counts)    print("")# randomSplit (not stratified)trainingData, testData = dataset.randomSplit([0.8, 0.2], seed = 100)trainingData.groupBy('label').count().show()print_class_balance(dataset, "total")print_class_balance(trainingData, "training")print_class_balance(testData, "test")# Exact stratification using Window (multi-class variant in comments)from pyspark.sql.window import *data = (    dataset    .withColumn("id", monotonically_increasing_id())    .withColumn("Random", rand())    .withColumn(        "Row",        row_number()        .over(            Window            .partitionBy("label")            .orderBy("Random")        )    ))training = data.where(    ((col("label") == 0) & (col("Row") < 392727 * 0.8)) |    ((col("label") == 1) & (col("Row") < 20566 * 0.8)))training.show()training.cache()#now we check the class of balance again print_class_balance(data, "total")print_class_balance(training, "training")print_class_balance(testData, "test")#PCAnumeric_features = [t[0] for t in dataset.dtypes if t[1] == 'double']vecAssembler = VectorAssembler(inputCols=numeric_features, outputCol="vec_features")standard_scaler = StandardScaler(inputCol=vecAssembler.getOutputCol(), outputCol="scaled_features")pca = PCA(k=6, inputCol=standard_scaler.getOutputCol(), outputCol="features")pipeline = Pipeline(stages=[vecAssembler, standard_scaler, pca])pipeline_model = pipeline.fit(training)training = pipeline_model.transform(training).select('GENRE', 'features', 'label')testData = pipeline_model.transform(testData).select('GENRE', 'features', 'label')#without pcafrom pyspark.ml.feature import VectorAssemblerassembler = VectorAssembler(    inputCols=[col for col in dataset.columns if col.startswith("M")],    outputCol="features")#feature1 = assembler.transform(dataset).select(["MSD_TRACKID", "Features"])#feature1.cache()training= training.withColumn('label', F.when(F.col('GENRE') == 'Rap', 1).otherwise(0))training = assembler.transform(training).select('GENRE', 'features', 'label')testData= testData.withColumn('label', F.when(F.col('GENRE') == 'Rap', 1).otherwise(0))testData = assembler.transform(testData).select('GENRE', 'features', 'label')# ------------# Downsampling# ------------training_downsampled = (    training    .withColumn("Random", rand())    .where((col("label") != 0) | ((col("label") == 0) & (col("Random") < 1 * ( 20566/ 392727)))))training_downsampled.count()training_downsampled.cache()#we once again check out the class balance of training data after down-samplingprint_class_balance(training_downsampled, "training")#define a function that returns performance metricsdef print_binary_metrics(predictions, labelCol="label", predictionCol="prediction", rawPredictionCol="rawPrediction"):    total = predictions.count()    positive = predictions.filter((col(labelCol) == 1)).count()    negative = predictions.filter((col(labelCol) == 0)).count()    nP = predictions.filter((col(predictionCol) == 1)).count()    nN = predictions.filter((col(predictionCol) == 0)).count()    TP = predictions.filter((col(predictionCol) == 1) & (col(labelCol) == 1)).count()    FP = predictions.filter((col(predictionCol) == 1) & (col(labelCol) == 0)).count()    FN = predictions.filter((col(predictionCol) == 0) & (col(labelCol) == 1)).count()    TN = predictions.filter((col(predictionCol) == 0) & (col(labelCol) == 0)).count()    binary_evaluator = BinaryClassificationEvaluator(rawPredictionCol=rawPredictionCol, labelCol=labelCol, metricName="areaUnderROC")    auroc = binary_evaluator.evaluate(predictions)    print('actual total:    {}'.format(total))    print('actual positive: {}'.format(positive))    print('actual negative: {}'.format(negative))    print('nP:              {}'.format(nP))    print('nN:              {}'.format(nN))    print('TP:              {}'.format(TP))    print('FP:              {}'.format(FP))    print('FN:              {}'.format(FN))    print('TN:              {}'.format(TN))    print('precision:       {}'.format(TP / (TP + FP)))    print('recall:          {}'.format(TP / (TP + FN)))    print('accuracy:        {}'.format((TP + TN) / total))    print('auroc:           {}'.format(auroc))#------------------------------------------------------------------before down-sampling #-----------------------lrlr = LogisticRegression()lr_model = lr.fit(training)lr_prediction = lr_model.transform(testData)print_binary_metrics(lr_prediction)#---------------------------random forest rf = RandomForestClassifier(labelCol="label", \                            featuresCol="features", \                            numTrees = 100, \                            maxDepth = 4, \                            maxBins = 32)# Train model with Training DatarfModel = rf.fit(training)rf_prediction = rfModel.transform(testData)print_binary_metrics(rf_prediction)#---------------------decision treefrom pyspark.ml.classification import DecisionTreeClassifierdt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)dt_model = dt.fit(training)dt_prediction = dt_model.transform(testData)print_binary_metrics(dt_prediction)#--------------------gbtfrom pyspark.ml.classification import GBTClassifiergbt = GBTClassifier(featuresCol='features', labelCol='label')gbt_model = dt.fit(training)gbt_prediction = dt_model.transform(testData)print_binary_metrics(gbt_prediction)#------------------------------------------------------------------after down-sampling training_downsampled= training_downsampled.withColumn('label', F.when(F.col('GENRE') == 'Rap', 1).otherwise(0))#-----------------------lrlr = LogisticRegression()lr_model = lr.fit(training_downsampled)lr_prediction = lr_model.transform(testData)print_binary_metrics(lr_prediction)#---------------------------random forest rf = RandomForestClassifier(labelCol="label", \                            featuresCol="features", \                            numTrees = 100, \                            maxDepth = 4, \                            maxBins = 32)# Train model with Training DatarfModel = rf.fit(training_downsampled)rf_prediction = rfModel.transform(testData)print_binary_metrics(rf_prediction)#---------------------decision treefrom pyspark.ml.classification import DecisionTreeClassifierdt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)dt_model = dt.fit(training_downsampled)dt_prediction = dt_model.transform(testData)print_binary_metrics(dt_prediction)#--------------------gbtfrom pyspark.ml.classification import GBTClassifiergbt = GBTClassifier(featuresCol='features', labelCol='label')gbt_model = dt.fit(training_downsampled)gbt_prediction = dt_model.transform(testData)print_binary_metrics(gbt_prediction)#-----------------------------------------cv#-------------------lrfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(lr_prediction)from pyspark.ml.tuning import ParamGridBuilder, CrossValidator# Create ParamGrid for Cross Validationlr_paramGrid = (ParamGridBuilder()             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features             .build())# Create 5-fold CrossValidatorcv_lr = CrossValidator(estimator=lr, \                    estimatorParamMaps=lr_paramGrid, \                    evaluator=evaluator, \                    numFolds=5)cv_lrModel = cv_lr.fit(training_downsampled)cv_lr_predictions = cv_lrModel.transform(testData)# Evaluate best modelevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(cv_lr_predictions)#-------------------rffrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(rf_prediction)from pyspark.ml.tuning import ParamGridBuilder, CrossValidator# Create ParamGrid for Cross Validationrf_paramGrid = (ParamGridBuilder()             .addGrid(rf.numTrees, [100, 200])              .addGrid(rf.maxDepth, [2, 4])              .addGrid(rf.maxBins, [16, 32])             .build())             # Create 5-fold CrossValidatorcv_rf = CrossValidator(estimator=rf, \                    estimatorParamMaps=rf_paramGrid, \                    evaluator=evaluator, \                    numFolds=5)cv_rfModel = cv_rf.fit(training_downsampled)cv_rf_predictions = cv_rfModel.transform(testData)# Evaluate best modelevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(cv_rf_predictions)#-----------------------------------------------------dtfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(dt_prediction)# Create ParamGrid for Cross Validationdt_paramGrid = (ParamGridBuilder()             .addGrid(dt.maxDepth, [2, 5, 10, 20, 30])             .addGrid(dt.maxBins, [10, 20, 40, 80, 100])             .build())# Create 5-fold CrossValidatorcv_dt = CrossValidator(estimator=dt, \                    estimatorParamMaps=dt_paramGrid, \                    evaluator=evaluator, \                    numFolds=5)cv_dtModel = cv_dt.fit(training_downsampled)cv_dt_predictions = cv_dtModel.transform(testData)# Evaluate best modelevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(cv_dt_predictions)#-----------------------------------------------------gbtfrom pyspark.ml.evaluation import MulticlassClassificationEvaluatorevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(gbt_prediction)gbt_paramGrid = (ParamGridBuilder()             .addGrid(gbt.maxDepth, [2, 4, 8])             .addGrid(gbt.maxBins, [24, 32, 48])             .addGrid(gbt.stepSize, [0.01, 0.1])             .build())             # Create 5-fold CrossValidatorcv_gbt = CrossValidator(estimator=gbt, \                    estimatorParamMaps=dt_paramGrid, \                    evaluator=evaluator, \                    numFolds=5)cv_gbtModel = cv_gbt.fit(training_downsampled)cv_gbt_predictions = cv_gbtModel.transform(testData)# Evaluate best modelevaluator = MulticlassClassificationEvaluator(predictionCol="prediction")evaluator.evaluate(cv_gbt_predictions)#-------------------------------q3#prepare the datasetdataset_q3= features_with_labels.drop("track_id")dataset_q3.groupBy('GENRE').count().show()dataset_q3.show()# create label indexlabel_stringIdx = StringIndexer(inputCol = "GENRE", outputCol = "label")data_labelindexed = label_stringIdx.fit(dataset_q3).transform(dataset_q3)#randomsplit train, test =  data_labelindexed.randomSplit([0.8, 0.2], seed=100)# create pipeline for transformationnumeric_features = [t[0] for t in dataset_q3.dtypes if t[1] == 'double']vecAssembler = VectorAssembler(inputCols=numeric_features, outputCol="vec_features")standard_scaler = StandardScaler(inputCol=vecAssembler.getOutputCol(), outputCol="scaled_features")pca = PCA(k=6, inputCol=standard_scaler.getOutputCol(), outputCol="features")pipeline = Pipeline(stages=[vecAssembler, standard_scaler, pca])pipeline_model = pipeline.fit(train)train = pipeline_model.transform(train).select('GENRE', 'features', 'label')test = pipeline_model.transform(test).select('GENRE', 'features', 'label')#class imbalance of the train data print_class_balance(train, "training")#-----------------------lrlr = LogisticRegression()lr_model = lr.fit(train)#f1 score lr_predictions = lr_model.transform(test)evaluator = MulticlassClassificationEvaluator()print(evaluator.getMetricName(), evaluator.evaluate(lr_predictions))print('accuracy',     evaluator.evaluate(lr_predictions, {evaluator.metricName: "accuracy"})) # 0.5658229256046017# cross validaition for logistic regessionparamGrid = (ParamGridBuilder()             .addGrid(lr.regParam, [0, 0.01, 0.1])              .addGrid(lr.elasticNetParam, [0, 0.1, 0.2])              .build())cv = CrossValidator(estimator=lr,                    estimatorParamMaps=paramGrid,                    evaluator=evaluator,                    numFolds=5)lr_cv_model = cv.fit(train)lr_cv_prediction = lr_cv_model.transform(test)print(evaluator.getMetricName(), evaluator.evaluate(lr_cv_prediction)) print('accuracy',     evaluator.evaluate(lr_cv_prediction, {evaluator.metricName: "accuracy"})) #---------------------------------------song recommendation #Q1 First it will be helpful to know more about the properties of the dataset before you being training the collaborative filtering model.#(a) How many unique songs are there in the dataset? How many unique users?triplets_not_mismatched.show()unique_songs = triplets_not_mismatched.dropDuplicates(['song_id'])unique_songs.count()unique_users = triplets_not_mismatched.dropDuplicates(['user_id'])unique_users.count()#(b) How many different songs has the most active user played?#What is this as a percentage of the total number of unique songs in the dataset?user_activity= triplets_not_mismatched.groupBy('user_id').agg(F.count('SONG_ID').alias('songs_played_by_users'), F.sum('plays').alias('no_plays_by_users'))user_activity.show()user_activity=user_activity.orderBy('no_plays_by_users', ascending = False) user_activity.show()#(c) Visualize the distribution of song popularity and the distribution of user activity. What is the shape of these distributions?song_popularity = triplets_not_mismatched.groupBy('song_id').agg(F.count('user_id').alias('no_users_for_songs'), F.sum('plays').alias('no_plays_for_songs')).orderBy('no_plays_for_songs', ascending = False)song_popularity.show()song_popularity_list = song_popularity.select('no_users_for_songs').rdd.map(lambda row:row[0]).collect() #get the no of users for songsuser_activity_list =user_activity.select('songs_played_by_users').rdd.map(lambda row:row[0]).collect() #get the no of songs played by users import numpy as npimport matplotlib.pyplot as pltdef plot_distribution(nums, title='Distribution', xlabel='plays'):    x = np.sort(np.array(nums))    y = np.arange(1, len(x)+1) / len(x)    plt.clf()    plt.plot(x, y, marker='.', linestyle='none')    plt.xlabel(xlabel)    plt.ylabel("Proportation")    plt.xlim((0,1000))    plt.title(title)    plt.savefig(title + '.jpg')plot_distribution(user_activity_list, 'Distribution of songs played', 'Users')plot_distribution(song_popularity_list, 'Distribution of users', 'Songs')#Create a clean dataset of user-song plays by removing songs which have been played less than N times and users who have listened to fewer than M songs in total. pop_songs = song_popularity.filter(song_popularity.no_users_for_songs>40)pop_songs.count()active_users = user_activity.filter(user_activity.songs_played_by_users>55)active_users.count()clean_user_songs = triplets_not_mismatched.join(pop_songs, on='song_id').join(active_users, on='user_id')clean_user_songs.show()# define a  functiondef rating_calculation(df, method='play_count'):    """return raing for model building    play_count: sue play_count directly as ratig    max_10: set all the play_count more than 10 to 10    divide_avg: divide play_count by average play count for each user    normalised_by_maximum: divide play_count by the maximum play count    """    if method == 'play_count':        ratings = df.withColumnRenamed('plays', 'rating')    elif method == 'max_10':        ratings = df.withColumn('rating',             F.when(F.col('plays')>10, 10).otherwise(F.col('plays')))    elif method == 'divide_avg':        ratings = df \            .withColumn('AVG_PLAY_COUNT_PER_SONG_BY_USER', \                F.col('SUM_PLAY_COUNT_BY_USER')/F.col('songs_played_by_users')) \            .withColumn('rating', F.col('plays')/F.col('AVG_PLAY_COUNT_PER_SONG_BY_USER'))    elif method == 'normalised_by_maximum':        taste_max= df.groupBy('user_id').agg(F.max('plays').alias('MAX_PLAY_COUNT'))        ratings = df \            .join(taste_max, on='user_id') \            .withColumn('rating', F.col('plays')/F.col('MAX_PLAY_COUNT'))    else:        print("wrong agument")    result = ratings.select('song_id', 'user_id', 'rating')    return resultratings = rating_calculation(clean_user_songs, method='play_count')ratings.show()# create integer index for user and songuserlabelIndexer = StringIndexer(inputCol='user_id', outputCol='user')songlabelIndexer = StringIndexer(inputCol='song_id', outputCol='item')pipeline = Pipeline(stages=[userlabelIndexer , songlabelIndexer])pipeline_model = pipeline.fit(ratings)data_transformed = pipeline_model.transform(ratings)data_transformed.show()# create a column holding number of rows in the window partitioned by userdata_ranked = data_transformed \    .withColumn("row_num",        F.row_number().over(Window.partitionBy("user").orderBy('song_id')))data_ranked.cache()data_ranked.count()data_ranked.show()# split dataset to training(80%) and testing data(20%) train = data_ranked.where((F.col("row_num")%5)!=0)train.show()train = train.repartition("user")test = data_ranked.where((F.col("row_num")%5)==0)print("testing dataset:")test.show()test = test.repartition("user")train.count()test.count()print('users in training dataset: ', train.dropDuplicates(['user']).count()) print('users in testing dataset: ', test.dropDuplicates(['user']).count()) print('users only in testing dataset: ',     train.join(test, how='left_anti').dropDuplicates(['user']).count()) #q2.  Next you will train the collaborative filtering model.# build ALS modelfrom pyspark.ml.recommendation import ALS, ALSModelfrom pyspark.ml.evaluation import RegressionEvaluatorfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidatorfrom pyspark.mllib.evaluation import RankingMetricsals = ALS(regParam=0.01, implicitPrefs=True)als_model = als.fit(train)als_prediction = als_model.transform(test)als_prediction.show()evaluator = RegressionEvaluator(metricName="rmse", labelCol="rating",                                predictionCol="prediction")evaluator.evaluate(als_prediction) #rmse#Select a few of the users from the test set by hand and use the model to generate some recommendations.windowSpec = Window.partitionBy('user').orderBy(F.col('prediction').desc())song_recommendation = als_prediction \    .select('user', 'item', 'prediction', F.rank().over(windowSpec).alias('rank')) \    .where(F.col('rank')<=5) \ #recommenend 5 items     .groupBy('user') \    .agg(F.collect_list('item').alias('song_recommended'))song_recommendation.cache()song_recommendation.show(5)test.where(test.user == "6.0").select("item")#Compare these recommendations to the songs the user has actually played.windowSpec = Window.partitionBy('user').orderBy(F.col('rating').desc())actually_played = test \    .select('user', 'rating', 'item', F.rank().over(windowSpec).alias('rank')) \    .where(F.col('rank')<=5) \    .groupBy('user') \    .agg(F.collect_list('item').alias('SONG_LISTENED'))actually_played.cache()actually_played.show()recommendation_and_listend = song_recommendation.join(actually_played, on='user') recommendation_and_listend.show()# calculate ranking metricssmetrics = RankingMetrics(    recommendation_and_listend \    .select(['song_recommended', 'SONG_LISTENED']) \    .rdd \    .map(lambda row: (row[0], row[1]))    )metrics.precisionAt(5) #precision metrics.ndcgAt(10) #NDCGmetrics.meanAveragePrecision #MAP